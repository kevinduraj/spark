{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Databricks notebook source exported at Sun, 21 Feb 2016 05:12:13 UTC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Check if reading all col, except the text + lowertext of the Parquet file, results in faster reading with less I/O via UI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " ### Live Demo: Wikipedia ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " We will: \n",
    "* Use a cluster with 50 Executors with 8 cores on each (400 cores total)\n",
    "* ETL from an XML file into a Parquet file\n",
    "* Work with nested fields in a table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " ##### Step 0: Ask students to run first 5 cells in \"Wikipedia - ETL NLP - ReadMe\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " ##### Step 1: Convert XML to Parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Start with using the Spark-XML library to convert the XML file to the more efficient Parquet format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(\"/mnt/wikipedia-readonly/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Note that the cell below will run 400 tasks at a time, each taking about 3.5 mins median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// This cell should take about 30-50 mins to run\n",
    "// Using Spark-xml1, it takes 50 mins, median 3.8 mins, 75th perct is 4.5 mins\n",
    "\n",
    "val wiki001DF = sqlContext.read\n",
    "    .format(\"com.databricks.spark.xml\")\n",
    "    .option(\"rowTag\", \"page\")\n",
    "    .load(\"/mnt/wikipedia-readonly/en_wikipedia/enwiki-20160204-pages-articles-multistream.xml\")\n",
    "    .write.parquet(\"/mnt/wikipedia-readwrite/en_wikipedia/parquetX/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " **Instructor note:** \n",
    "* Under Executors tab, show that each Executor is reading an equal amount of data, around 1 GB\n",
    "* Show Event Timeline\n",
    "* Thread dump will show that the most time is spent in these two libraries:\n",
    "\n",
    "`com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl$FragmentContentDriver.next(XMLDocumentFragmentScannerImpl.java:2819)`\n",
    "\n",
    "or \n",
    "\n",
    "`java.util.zip.ZipFile.getEntry(ZipFile.java:309)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%fs ls /mnt/wikipedia-readonly/en_wikipedia/parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " ##### Step 2: Read Parquet into a Dataframe and explore..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Read the new optimized Parquet file into a Dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val wikiDF = sqlContext.read.parquet(\"/mnt/wikipedia-readonly/en_wikipedia/parquet\").cache() //We are caching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Notice the nested schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wikiDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " There are only 6 high-level columns (the rest are nested fields):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wikiDF.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Here's an example of how to read nested data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Note: If a user is logged in, we don't get an IP\n",
    "wikiDF.select($\"id\", $\"title\", $\"revision.contributor.username\", $\"revision.contributor.ip\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// These are the articles last touched by anonymous editors\n",
    "wikiDF.filter(\"revision.contributor.ip is not null\").select($\"id\", $\"title\", $\"revision.contributor.username\", $\"revision.contributor.ip\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " ##### Step 3: Materialize the cache and sanity check the data for namespaces and redirects\n",
    "\n",
    "Are there really 5 million articles?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "//materialize the cache and count how many rows (takes 9 secs to run)\n",
    "wikiDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Hmm, why are there 16 million rows? I thought English Wikipedia had 5 million articles..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Wikipedia namespaces: https://en.wikipedia.org/wiki/Wikipedia:Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wikiDF.groupBy(\"ns\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Ahh, there are many other namespaces in this Dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Filter the Dataframe down to just the main namespace of articles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val wikiMainDF = wikiDF.filter(\"ns = 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wikiMainDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " There are still way too many rows... 12 million, instead of 5 million."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Notice the redirect column\n",
    "wikiMainDF.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Try going to: https://en.wikipedia.org/wiki/AccessibleComputing    This is not a real article, just redir\n",
    "\n",
    "// This is a real article: https://en.wikipedia.org/wiki/Anarchism\n",
    "\n",
    "// Notice that many of the rows are just redirects\n",
    "\n",
    "display(wikiMainDF.select($\"id\", $\"title\", $\"redirect.@title\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Now we see that there are 5 million normal articles that are not redirects\n",
    "\n",
    "wikiMainDF.select($\"redirect.@title\".isNotNull.as(\"hasRedirect\"))\n",
    "  .groupBy(\"hasRedirect\")\n",
    "  .count\n",
    "  .show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Create a new wikiArticlesDF with just the 5 million articles, removing the redirect rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val wikiArticlesDF = wikiMainDF.filter($\"redirect.@title\".isNull)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// This makes sense, 5 million articles\n",
    "wikiArticlesDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " ##### Step 4: Convert the String timestamp cols to real timestamp data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wikiArticlesDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wikiArticlesDF.select($\"title\", $\"revision.timestamp\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.{functions => func}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's use a function for time zone manipulation and to store the relavent fields as a timestamp rather than a string.  Let's use `from_utc_timestamp` to get a timestamp object back with the correct time zone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// We are using this function added in Spark 1.5: https://issues.apache.org/jira/browse/SPARK-8188\n",
    "\n",
    "// https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val wikiArticlesWTimeDF = wikiArticlesDF.withColumn(\"lastrev_est_time\", func.from_utc_timestamp($\"revision.timestamp\", \"US/Eastern\"))\n",
    "\n",
    "wikiArticlesWTimeDF.printSchema\n",
    "\n",
    "wikiArticlesWTimeDF\n",
    "  .select($\"title\", $\"revision.timestamp\", $\"lastrev_est_time\")\n",
    "  .show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Notice that the lastrev_est_time column is now a timestamp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " ##### Step 5: Flatten the table and drop unnecessary columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Flatten out the table and drop some cols:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val wikiFlatForParquetDF = wikiArticlesWTimeDF\n",
    "                      .drop($\"ns\")\n",
    "                      .drop($\"redirect\")\n",
    "                      .drop($\"restrictions\")\n",
    "                      .withColumn(\"revid\", $\"revision.id\")\n",
    "                      .withColumn(\"comment\", $\"revision.comment.#value\")\n",
    "                      .withColumn(\"contributorid\", $\"revision.contributor.id\")\n",
    "                      .withColumn(\"contributorusername\", $\"revision.contributor.username\")\n",
    "                      .withColumn(\"contributorip\", $\"revision.contributor.ip\")\n",
    "                      .withColumn(\"text\", $\"revision.text.#value\")\n",
    "                      .withColumn(\"comment\", $\"revision.comment.#value\")\n",
    "                      .drop($\"revision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wikiFlatForParquetDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " ##### Step 6: Write full data to Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// takes 93 sec on 45W8c\n",
    "// takes 108 sec on 30W8c\n",
    "// takes 1 min to write on 50W8C\n",
    "wikiFlatForParquetDF.write.parquet(\"/mnt/wikipedia-readwrite/en_wikipedia/flattenedParquet/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " ##### Step 7: Keep only the articles that were last updated during or after 2016 and write that smaller subset to Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Import the sql functions package, which includes statistical functions like sum, max, min, avg, etc.\n",
    "import org.apache.spark.sql.functions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wikiFlatForParquetDF.filter(year($\"lastrev_est_time\") >= 2016).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Over 1 million articles were last updated since the beginning of 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wikiFlatForParquetDF.filter(year($\"lastrev_est_time\") >= 2016).write.parquet(\"/mnt/wikipedia-readwrite/en_wikipedia/flattenedParquet_updated2016/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " ### Write a 1% sample of the 1 million words for students' lab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val wikiFlatDFx = sqlContext.read.parquet(\"dbfs:/mnt/wikipedia-readonly/en_wikipedia/flattenedParquet_updated2016/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.RegexTokenizer\n",
    " \n",
    "val tokenizer = new RegexTokenizer()\n",
    "  .setInputCol(\"text\")\n",
    "  .setOutputCol(\"words\")\n",
    "  .setPattern(\"\\W+\")\n",
    "\n",
    "val wikiWordsDFx = tokenizer.transform(wikiFlatDFx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val onePercentDFx = wikiWordsDFx\n",
    "                      .sample(false, .01, 555)\n",
    "                      .repartition(100)\n",
    "                      .write\n",
    "                      .parquet(\"/mnt/wikipedia-readwrite/en_wikipedia/flattenedParquet_updated2016_1percent/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "onePercentDFx.write.parquet(\"/mnt/wikipedia-readwrite/en_wikipedia/flattenedParquet_updated2016_1percent/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " ### More ETL and NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Restart Cluster before continuing...(different sql.functions import)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val wikiFlat5milDF = sqlContext.read.parquet(\"/mnt/wikipedia-readwrite/en_wikipedia/flattenedParquet/\").cache\n",
    "wikiFlat5milDF.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " ##### Step 1: Natural Language Processing: lowercase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Next, let's convert the text field to lowercase.  We'll use the `lower` function for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wikiFlat5milDF.select($\"text\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val wikiFlat5milLoweredDF = wikiFlat5milDF.select($\"*\", lower($\"text\").as(\"lowerText\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wikiFlat5milLoweredDF.select($\"text\", $\"lowerText\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " ##### Step 2: NLP: Convert the lowerText column into a bag of words and remove stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Next, let's convert our text into a list of words so that we can perform some analysis at the word level.  For this will use a feature transformer called `RegexTokenizer` which splits up strings into tokens (words in our case) based on a split pattern.  We'll split our text on anything that matches one or more non-word characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.RegexTokenizer\n",
    " \n",
    "val tokenizer = new RegexTokenizer()\n",
    "  .setInputCol(\"lowerText\")\n",
    "  .setOutputCol(\"words\")\n",
    "  .setPattern(\"\\W+\")\n",
    "\n",
    "val wikiWordsDF = tokenizer.transform(wikiFlat5milLoweredDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wikiWordsDF.select(\"words\").first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "There are some very common words in our list of words which won't be that useful for our later analysis.  We'll create a UDF to remove them.\n",
    " \n",
    "[StopWordsRemover](http://spark.apache.org/docs/latest/ml-features.html#stopwordsremover) is implemented for Scala but not yet for Python.  We'll use the same [list](http://ir.dcs.gla.ac.uk/resources/linguistic_utils/stop_words) of stop words it uses to build a user-defined function (UDF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val stopWords = sc.textFile(\"/mnt/wikipedia-readonly/stopwords/stop_words.txt\").collect.toSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Create a custom, more powerful function to remove stop words (including words with length < 3 and words containing a digit or underscore):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scala.collection.mutable.WrappedArray\n",
    " \n",
    "val stopWordsBroadcast = sc.broadcast(stopWords)  // Notice we're using a broadcast variable\n",
    " \n",
    "def isDigitOrUnderscore(c: Char) = {\n",
    "    Character.isDigit(c) || c == '_'\n",
    "}\n",
    " \n",
    "def keepWord(word: String) = word match {\n",
    "    case x if x.length < 3 => false\n",
    "    case x if stopWordsBroadcast.value(x) => false\n",
    "    case x if x exists isDigitOrUnderscore => false\n",
    "    case _ => true\n",
    "}\n",
    " \n",
    "def removeWords(words: WrappedArray[String]) = {\n",
    "    words.filter(keepWord(_))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Test the function locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "removeWords(Array(\"test\", \"cat\", \"do343\", \"343\", \"spark\", \"the\", \"and\", \"hy-phen\", \"under_score\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Create a UDF from our function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.udf\n",
    "val removeWordsUDF = udf { removeWords _ }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Register this function so that we can call it later from another notebook.  Note that in Scala `register` also returns a `udf` that we can use, so we could have combined the above step into this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlContext.udf.register(\"removeWords\", removeWords _)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Apply our function to the `wikiWordsDF` `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val wikiCleanedDF = wikiWordsDF\n",
    "  .withColumn(\"noStopWords\", removeWordsUDF($\"words\"))\n",
    "  .drop(\"words\")\n",
    "  .withColumnRenamed(\"noStopWords\", \"words\")\n",
    " \n",
    "wikiCleanedDF.select(\"words\").take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Let's see what the top 15 words are now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val noStopWordsListAllWikiDF = wikiCleanedDF.select(explode($\"words\").as(\"word\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "noStopWordsListAllWikiDF.show(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "noStopWordsListAllWikiDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " That's 2.7 billion words!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Finally, let's see the top 15 words in all of Wikipedia now (with the stop words removed):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val noStopWordsGroupCount = noStopWordsListAllWikiDF\n",
    "                      .groupBy(\"word\")  // group\n",
    "                      .agg(count(\"word\").as(\"counts\"))  // aggregate\n",
    "                      .sort(desc(\"counts\"))  // sort\n",
    "\n",
    "noStopWordsGroupCount.take(15).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Hmm, that looks better than the list we say when working with just 10,000 articles and using the Spark.ML built in stop words remover (which left words like 1, 2, s, etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " ##### Step 3: Write the cleaned dataframe to a new Parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// takes 93 sec on 45W8c\n",
    "// takes 108 sec on 30W8c\n",
    "// takes 1 min to write on 50W8C\n",
    "wikiCleanedDF.write.parquet(\"/mnt/wikipedia-readonly/en_wikipedia/cleanedParquet/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " ## Machine Learning Pipeline: TF-IDF and K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "//val wikiCleanedDF = sqlContext.read.parquet(\"/mnt/wikipedia-readonly/en_wikipedia/cleanedParquet/\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wikiCleanedDF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wikiCleanedDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " #### Set up the ML Pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.{RegexTokenizer, StopWordsRemover, IDF, HashingTF, Normalizer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "/*val tokenizer = new RegexTokenizer()\n",
    "  .setInputCol(\"lowerText\")\n",
    "  .setOutputCol(\"words2\")\n",
    "  .setPattern(\"\\W+\")\n",
    "  */"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// There are probably > 20K unique words\n",
    "// More features = more complexity and computational time and accucaracy\n",
    "\n",
    "val hashingTF = new HashingTF().setInputCol(\"words\").setOutputCol(\"hashingTF\").setNumFeatures(20000)\n",
    "val featurizedData = hashingTF.transform(wikiCleanedDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val idf = new IDF().setInputCol(\"hashingTF\").setOutputCol(\"idf\")\n",
    "val idfModel = idf.fit(featurizedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// A normalizer is a common operation for text classification.\n",
    "\n",
    "// It simply gets all of the data on the same scale... for example, if one article is much longer and another, it'll normalize the scales for the different features.\n",
    "\n",
    "// If we don't normalize, an article with more words would be weighted differently\n",
    "\n",
    "\n",
    "val normalizer = new Normalizer()\n",
    "  .setInputCol(\"idf\")\n",
    "  .setOutputCol(\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now, let's build the `KMeans` estimator and a `Pipeline` that will contain all of the stages.  We'll then call fit on the `Pipeline` which will give us back a `PipelineModel`.  This will take about a minute to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "//for k = 50 takes 7 mins on a VeryLarge 60 worker, 8 core cluster\n",
    "\n",
    "//for k = 100, 11 mins to run on VeryLarge\n",
    "\n",
    "// 30W8c can run 120 tasks simultaneously, 15.7 mins (4 cores really each Exec)\n",
    "// 45W8c can run 180 tasks  simul, 13.3 mins (4 cores really each Exec)\n",
    "\n",
    "// %sql SET spark.sql.shuffle.partitions\n",
    "\n",
    "// On a 50 Worker cluster, this takes... 757 sec / 60 = 12 mins\n",
    "\n",
    "import org.apache.spark.ml.Pipeline\n",
    "import org.apache.spark.ml.clustering.KMeans\n",
    " \n",
    "val kmeans = new KMeans()\n",
    "  .setFeaturesCol(\"features\")\n",
    "  .setPredictionCol(\"prediction\")\n",
    "  .setK(100)\n",
    "  .setSeed(0) // for reproducability\n",
    " \n",
    "val pipeline = new Pipeline()\n",
    "  .setStages(Array(hashingTF, idf, normalizer, kmeans))  //add tokenizer, then stop workds remover back later for demo2\n",
    " \n",
    "val model = pipeline.fit(wikiCleanedDF)\n",
    "\n",
    "// This should kick off 33 jobs and take 20 mins to run (1257 sec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Show DAG visualization while we wait."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " The above ML pipeline costs under $50 to run.\n",
    "\n",
    "Spot prices are set by Amazon EC2 and fluctuate periodically depending on the supply of and demand for Spot instance capacity.\n",
    "\n",
    "On Demand: ($0.66 per r3.2xlarge machine * 50)\n",
    "https://aws.amazon.com/ec2/pricing/\n",
    "\n",
    "Spot: ($0.07 per r3.2xlarge machine * 50)\n",
    "https://aws.amazon.com/ec2/spot/pricing/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's take a look at a sample of the data to see if we can see a pattern between predicted clusters and titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val predictionsDF = model.transform(wikiCleanedDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictionsDF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictionsDF.groupBy(\"prediction\").count().show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "//politics\n",
    "display(predictionsDF.filter(\"prediction = 16\").select(\"title\", \"prediction\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// This cluster seems to be about Ford, but notice that TF-IDF can't tell between difference the car and last name\n",
    "\n",
    "//Name:  https://en.wikipedia.org/wiki/Whitey_Ford\n",
    "// https://en.wikipedia.org/wiki/Harrison_Ford\n",
    "\n",
    "display(predictionsDF.filter(\"prediction = 70\").select(\"title\", \"prediction\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "//Norway, nordic\n",
    "display(predictionsDF.filter(\"prediction = 40\").withColumn(\"num_words\", size($\"words\")).select(\"title\", \"num_words\", \"prediction\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Games and Sports\n",
    "display(predictionsDF.filter(\"prediction = 96\").withColumn(\"num_words\", size($\"words\")).select(\"title\", \"num_words\", \"prediction\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Let's find which cluster Apache_Spark is in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Looking for Spark\n",
    "display(predictionsDF.filter($\"title\" === \"Apache Spark\").withColumn(\"num_words\", size($\"words\")).select(\"title\", \"num_words\", \"prediction\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// What will the cluster contain? big data? technology? software?\n",
    "display(predictionsDF.filter(\"prediction = 8\").withColumn(\"num_words\", size($\"words\")).select(\"title\", \"num_words\", \"prediction\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark-DSE Cluster",
   "language": "scala",
   "name": "spark-dse-cluster"
  },
  "language_info": {
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
