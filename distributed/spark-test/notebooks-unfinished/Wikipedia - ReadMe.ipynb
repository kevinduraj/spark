{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "// Databricks notebook source exported at Sun, 21 Feb 2016 05:12:02 UTC\n",
    "\n",
    "\n",
    "#![Wikipedia Logo](http://sameerf-dbc-labs.s3-website-us-west-2.amazonaws.com/data/wikipedia/images/w_logo_for_labs.png)\n",
    "\n",
    "# Explore English Wikipedia via DataFrames and RDD API\n",
    "### Time to complete: 20 minutes\n",
    "\n",
    "#### Business Questions:\n",
    "\n",
    "* Question # 1) What percentage of Wikipedia articles were edited in the past month (before the data was collected)?\n",
    "* Question # 2) How many of the 1 million articles were last edited by ClueBot NG, an anti-vandalism bot?\n",
    "* Question # 3) Which user in the 1 million articles was the last editor of the most articles?\n",
    "* Question # 4) Can you display the titles of the articles in Wikipedia that contain a particular word?\n",
    "* Question # 5) Can you extract out all of the words from the Wikipedia articles? (bag of words)\n",
    "* Question # 6) What are the top 15 most common words in the English language?\n",
    "* Question # 7) After removing stop words, what are the top 10 most common words in the english language? \n",
    "* Question # 8) How many distinct/unique words are in noStopWordsListDF?\n",
    "\n",
    "\n",
    "#### Technical Accomplishments:\n",
    "\n",
    "* Work with one fifth of the sum of all human knowledge!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Attach to, and then restart your cluster first to clear out old caches and get to a default, standard environment. The restart should take 1 - 2 minutes.\n",
    "\n",
    "#![Restart cluster](http://i.imgur.com/xkRjRYy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Getting to know the Data\n",
    "Let's pick up where the instructor left off in the earlier demo. Locate the Parquet data from the demo using `dbutils`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(\"/mnt/wikipedia-readonly/en_wikipedia/flattenedParquet_updated2016/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " These are the ~840 parquet files (~4.3 GB) from the English Wikipedia Articles (Feb 4, 2016 snapshot) that were last updated in 2016."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Load the articles into memory and lazily cache them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val wikiDF = sqlContext.read.parquet(\"dbfs:/mnt/wikipedia-readonly/en_wikipedia/flattenedParquet_updated2016/\").cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Notice how fast `printSchema()` runs... this is because we can derive the schema from the Parquet metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wikiDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Look at the first 5 rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wikiDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Let's count how many total articles we have. (Note that when using a local mode cluster, the next command will take **4 minutes**, so you may want to skip ahead and read some of the next cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// You can monitor the progress of this count + cache materialization via the Spark UI's storage tab\n",
    "wikiDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " ## During live ETL demo: Run everything above this cell!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " This lab is meant to introduce you to working with unstructured text data in the Wikipedia articles. DataFrames and SQL queries are great for structured data like CSV, JSON or parquet files. However, when exploring unstructured data, using the RDD or Datasets API directly could give you more flexible, lower level control. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " In this lab, among other tasks, we will continue the ETL process from the earlier demo and apply basic Natural Language Processing to the article text to extract out a bag of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " By now the `.count()` operation might be completed. Go back up and check and only proceed after the count has completed. You should see the count's results as 1,029,377 items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Check the Spark UI's Storage tab to ensure that 100% of the data set fits in memory:\n",
    "\n",
    "#![memory](http://i.imgur.com/YUhs1Bz.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Run `.count()` again to see the speed increase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wikiDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " That's pretty impressive! We can scan through 1 million recent articles of English Wikipedia using a single 22 GB Executor in under 2 seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Register the DataFrame as a temporary table, so we can execute SQL against it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wikiDF.registerTempTable(\"wikipedia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " \n",
    "### Question #1:\n",
    "** What percentage of Wikipedia articles were edited in the past week (before the data was collected)? **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Recall that our dataset was collected on Feb 4, 2016. Let's figure out how many of the articles were last edited between Jan 28, 2016 - Feb 4, 2016. This should give us a good idea of how many articles are \"fresh\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " ** Challenge 1:**  Can you write this query using SQL? Hint: Just count all the articles where the last revision time is greater than Jan 28, 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%sql SELECT COUNT(*) FROM wikipedia WHERE lastrev_est_time >= DATE '2016-01-28';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " 315 thousand articles are less than a month old. Since English Wikipedia contains 5,072,474 articles, that means:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "315931/5072474.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " About 6% of English Wikipedia is less than 1 week old (from the Feb 4th collection date). Here are 10 such articles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%sql SELECT title, lastrev_est_time FROM wikipedia WHERE lastrev_est_time >= DATE '2016-01-28' LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " \n",
    "### Question #2:\n",
    "** How many of the 1 million articles were last edited by [ClueBot NG](https://en.wikipedia.org/wiki/User:ClueBot_NG), an anti-vandalism bot? **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " ** Challenge 2:**  Write a SQL query to answer this question. The username to search for is `ClueBot BG`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%sql SELECT COUNT(*) FROM wikipedia WHERE contributorusername = \"ClueBot NG\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%sql SELECT * FROM wikipedia WHERE contributorusername = \"ClueBot NG\" LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " You can study at the specifc revisions like so: https://en.wikipedia.org/?diff=#\n",
    "\n",
    "For example: https://en.wikipedia.org/?diff=702283675"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " \n",
    "### Question #3:\n",
    "** Which user in the 1 million articles was the last editor of the most articles? **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Here's a slightly more complicated query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%sql SELECT contributorusername, COUNT(contributorusername) FROM wikipedia GROUP BY contributorusername ORDER BY COUNT(contributorusername) DESC; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Hmm, looks are bots are quite active in maintaining Wikipedia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Interested in learning more about the bots that edit Wikipedia? Check out: https://en.wikipedia.org/wiki/Wikipedia:List_of_bots_by_number_of_edits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " \n",
    "### Question #4:\n",
    "** Can you display the titles of the articles in Wikipedia that contain a particular word? **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Start by registering a User Defined Function (UDF) that can search for a string in the text of an article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Register a function that can search that a string is found.\n",
    "\n",
    "val containsWord = (s: String, w: String) => {\n",
    "  (s != null && s.indexOfSlice(w) >= 0).toString()\n",
    "}\n",
    "sqlContext.udf.register(\"containsWord\", containsWord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Verify that the `containsWord` function is working as intended:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Look for the word 'test' in the first string\n",
    "containsWord(\"hello astronaut, how's space?\", \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Look for the word 'space' in the first string\n",
    "containsWord(\"hello astronaut, how's space?\", \"space\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "  Use a parameterized query so you can easily change the word to search for:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%sql  select title from wikipedia where containsWord(text, '$word') == 'true'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Try typing in `NASA` or `Manhattan` into the search box above and hit SHIFT + ENTER."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " \n",
    "### Question #5:\n",
    "** Can you extract out all of the words from the Wikipedia articles? ** (Create a bag of words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Use Spark.ml's RegexTokenizer to read an input column of 'text' and write a new output column of 'words':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.RegexTokenizer\n",
    " \n",
    "val tokenizer = new RegexTokenizer()\n",
    "  .setInputCol(\"text\")\n",
    "  .setOutputCol(\"words\")\n",
    "  .setPattern(\"\\W+\")\n",
    "val wikiWordsDF = tokenizer.transform(wikiDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wikiWordsDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wikiWordsDF.select($\"title\", $\"words\").first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " \n",
    "### Question #6:\n",
    "** What are the top 15 most common words in the English language? ** Compute this only on a random 1% of the 1 million articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " For this analysis, we should get reasonably accurate results even if we work on just 1% of the 1 million articles. Plus, this will speed things up tremendously. Note that 1% of 1 million is 10,000 articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// This sample + repartition command will take 4-5 mins to run, so skip this cell and just read the same results via the parquet file in the following cell\n",
    "//val onePercentDF = wikiWordsDF.sample(false, .01, 555).repartition(100).cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val onePercentDF = sqlContext.read.parquet(\"dbfs:/mnt/wikipedia-readonly/en_wikipedia/flattenedParquet_updated2016_1percent/\").cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "onePercentDF.count // Materialize the cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " The `onePercentDF` contains 10,297 articles (that is 1% of 1 million articles). Take a look at the onePercentDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "display(onePercentDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Note that the `words` column contains arrays of Strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "onePercentDF.select($\"words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Let's explode the `words` column into a table of one word per row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.{functions => func}\n",
    "val onePercentWordsListDF = onePercentDF.select(func.explode($\"words\").as(\"word\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "display(onePercentWordsListDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "onePercentWordsListDF.cache().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " The onePercentWordsListDF contains 18.6 million words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Finally, run a wordcount on the exploded table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val wordGroupCountDF = onePercentWordsListDF\n",
    "                      .groupBy(\"word\")  // group\n",
    "                      .agg(func.count(\"word\").as(\"counts\"))  // aggregate\n",
    "                      .sort(func.desc(\"counts\"))  // sort\n",
    "\n",
    "wordGroupCountDF.take(15).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " These would be good [stop words](https://en.wikipedia.org/wiki/Stop_words) to filter out before running Natural Language Processing algorithms on our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " \n",
    "### Question #7:\n",
    "** After removing stop words, what are the top 10 most common words in the english language? ** Compute this only on a random 1% of the 1 million articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Use Spark.ml's stop words remover:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.StopWordsRemover\n",
    "\n",
    "val remover = new StopWordsRemover()\n",
    "  .setInputCol(\"words\")\n",
    "  .setOutputCol(\"noStopWords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Notice the removal of words like \"about\", \"the\",  etc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "remover.transform(onePercentDF).select(\"id\", \"title\", \"words\", \"noStopWords\").show(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val noStopWordsListDF = remover.transform(onePercentDF).select(func.explode($\"noStopWords\").as(\"word\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "noStopWordsListDF.show(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " The onePercentWordsListDF (which included stop words) contained 18.6 million words. How many words are in the noStopWordsListDF?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "noStopWordsListDF.cache.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " 13.9 million words remain. That means about 4.7 million words in our 1% sample were actually stop words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Finally, let's see the top 15 words now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val noStopWordsGroupCount = noStopWordsListDF\n",
    "                      .groupBy(\"word\")  // group\n",
    "                      .agg(func.count(\"word\").as(\"counts\"))  // aggregate\n",
    "                      .sort(func.desc(\"counts\"))  // sort\n",
    "\n",
    "noStopWordsGroupCount.take(15).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Hmm, there are still some words in the list (like http, 1, 2, s) that are kind of meaningless. Perhaps we should consider a custom stop words remover in the future?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " \n",
    "### Question #8:\n",
    "** How many distinct/unique words are in noStopWordsListDF?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "noStopWordsListDF.distinct.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Looks like the Wikipedia corpus has around 500,000 unique words. Probably a lot of these are rare scientific words, numbers, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " This concludes the English Wikipedia NLP lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark-DSE Cluster",
   "language": "scala",
   "name": "spark-dse-cluster"
  },
  "language_info": {
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
