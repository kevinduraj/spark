{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "// Databricks notebook source exported at Wed, 10 Feb 2016 20:42:12 UTC\n",
    "\n",
    "\n",
    "#![Wikipedia Logo](http://sameerf-dbc-labs.s3-website-us-west-2.amazonaws.com/data/wikipedia/images/w_logo_for_labs.png)\n",
    "\n",
    "# Explore English Wikipedia pageviews by second\n",
    "### Time to complete: 15 minutes\n",
    "\n",
    "#### Business questions:\n",
    "\n",
    "* Question # 1) How many rows in the table refer to *mobile* vs *desktop* site requests?\n",
    "* Question # 2) How many total incoming requests were to the *mobile* site vs the *desktop* site?\n",
    "* Question # 3) What is the start and end range of time for the pageviews data? How many days total of data do we have?\n",
    "* Question # 4) Which day of the week does Wikipedia get the most traffic?\n",
    "* Question #  5) Can you visualize both the mobile and desktop site requests together in a line chart to compare traffic between both sites by day of the week?\n",
    "\n",
    "#### Technical Accomplishments:\n",
    "\n",
    "* Use Spark's Scala and Python APIs\n",
    "* Learn what a `sqlContext` is and how to use it\n",
    "* Load a 255 MB tab separated file into a DataFrame\n",
    "* Cache a DataFrame into memory\n",
    "* Run some DataFrame transformations and actions to create visualizations\n",
    "* Learn the following DataFrame operations: `show()`, `printSchema()`, `orderBy()`, `filter()`, `groupBy()`, `cast()`, `alias()`, `distinct()`, `count()`, `sum()`, `avg()`, `min()`, `max()`\n",
    "* Write a User Defined Function (UDF)\n",
    "* Join two DataFrames\n",
    "* Bonus: Use Matplotlib and Python code within a Scala notebook to create a line chart\n",
    "\n",
    "\n",
    "\n",
    "Dataset: http://datahub.io/en/dataset/english-wikipedia-pageviews-by-second"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Introduction to running Scala in Databricks Notebooks\n",
    "\n",
    "Place your cursor inside the cells below, one at a time, and hit \"Shift\" + \"Enter\" to execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// This is a Scala cell. You can run normal Scala code here...\n",
    "val x = 1 + 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Here is another Scala cell, that adds 2 to x\n",
    "val y = 2 + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y is equal to 10\n"
     ]
    }
   ],
   "source": [
    "// This line uses string interpolation to prints what y is equal to...\n",
    "println(s\"y is equal to ${y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This was last run on: Fri Feb 12 19:44:21 PST 2016\n"
     ]
    }
   ],
   "source": [
    "// You can import additional modules and use them\n",
    "import java.util.Date\n",
    "println(s\"This was last run on: ${new Date}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### DataFrames\n",
    "A `sqlContext` object is your entry point for working with structured data (rows and columns) in Spark.\n",
    "\n",
    "Let's use the `sqlContext` to read a table of the English Wikipedia pageviews per second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.hive.HiveContext = org.apache.spark.sql.hive.HiveContext@4930213b"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Notice that the sqlContext in Databricks is actually a HiveContext\n",
    "sqlContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " A `HiveContext` includes additional features like the ability to write queries using the more complete HiveQL parser, access to Hive UDFs, and the ability to read data from Hive tables. In general, you should always aim to use the `HiveContext` over the more limited `sqlContext`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Create a DataFrame named `pageviewsDF` and understand its schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.sql.catalyst.analysis.NoSuchTableException\n",
       "Message: null\n",
       "StackTrace: org.apache.spark.sql.hive.client.ClientInterface$$anonfun$getTable$1.apply(ClientInterface.scala:112)\n",
       "org.apache.spark.sql.hive.client.ClientInterface$$anonfun$getTable$1.apply(ClientInterface.scala:112)\n",
       "scala.Option.getOrElse(Option.scala:120)\n",
       "org.apache.spark.sql.hive.client.ClientInterface$class.getTable(ClientInterface.scala:112)\n",
       "org.apache.spark.sql.hive.client.ClientWrapper.getTable(ClientWrapper.scala:60)\n",
       "org.apache.spark.sql.hive.HiveMetastoreCatalog.lookupRelation(HiveMetastoreCatalog.scala:227)\n",
       "org.apache.spark.sql.hive.HiveContext$$anon$2.org$apache$spark$sql$catalyst$analysis$OverrideCatalog$$super$lookupRelation(HiveContext.scala:371)\n",
       "org.apache.spark.sql.catalyst.analysis.OverrideCatalog$$anonfun$lookupRelation$3.apply(Catalog.scala:165)\n",
       "org.apache.spark.sql.catalyst.analysis.OverrideCatalog$$anonfun$lookupRelation$3.apply(Catalog.scala:165)\n",
       "scala.Option.getOrElse(Option.scala:120)\n",
       "org.apache.spark.sql.catalyst.analysis.OverrideCatalog$class.lookupRelation(Catalog.scala:165)\n",
       "org.apache.spark.sql.hive.HiveContext$$anon$2.lookupRelation(HiveContext.scala:371)\n",
       "org.apache.spark.sql.DataFrameReader.table(DataFrameReader.scala:276)\n",
       "$line31.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:21)\n",
       "$line31.$read$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:26)\n",
       "$line31.$read$$iwC$$iwC$$iwC$$iwC.<init>(<console>:28)\n",
       "$line31.$read$$iwC$$iwC$$iwC.<init>(<console>:30)\n",
       "$line31.$read$$iwC$$iwC.<init>(<console>:32)\n",
       "$line31.$read$$iwC.<init>(<console>:34)\n",
       "$line31.$read.<init>(<console>:36)\n",
       "$line31.$read$.<init>(<console>:40)\n",
       "$line31.$read$.<clinit>(<console>)\n",
       "$line31.$eval$.<init>(<console>:7)\n",
       "$line31.$eval$.<clinit>(<console>)\n",
       "$line31.$eval.$print(<console>)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "java.lang.reflect.Method.invoke(Method.java:497)\n",
       "org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\n",
       "org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1338)\n",
       "org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\n",
       "org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\n",
       "org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\n",
       "com.ibm.spark.interpreter.ScalaInterpreter$$anonfun$interpretAddTask$1$$anonfun$apply$3.apply(ScalaInterpreter.scala:304)\n",
       "com.ibm.spark.interpreter.ScalaInterpreter$$anonfun$interpretAddTask$1$$anonfun$apply$3.apply(ScalaInterpreter.scala:299)\n",
       "com.ibm.spark.global.StreamState$.withStreams(StreamState.scala:80)\n",
       "com.ibm.spark.interpreter.ScalaInterpreter$$anonfun$interpretAddTask$1.apply(ScalaInterpreter.scala:298)\n",
       "com.ibm.spark.interpreter.ScalaInterpreter$$anonfun$interpretAddTask$1.apply(ScalaInterpreter.scala:298)\n",
       "com.ibm.spark.utils.TaskManager$$anonfun$add$2$$anon$1.run(TaskManager.scala:123)\n",
       "java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
       "java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
       "java.lang.Thread.run(Thread.java:745)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Note that we have pre-loaded the pageviews_by_second data into Databricks.\n",
    "// You just have to read the existing table.\n",
    "\n",
    "val pageviewsDF = sqlContext.read.table(\"pageviews_by_second\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Compile Error\n",
       "Message: <console>:18: error: not found: value pageviewsDF\n",
       "              pageviewsDF.show()\n",
       "              ^\n",
       "StackTrace: "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Shows the first 20 records in ASCII print\n",
    "pageviewsDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Compile Error\n",
       "Message: <console>:19: error: not found: value display\n",
       "              display(pageviewsDF)\n",
       "              ^\n",
       "StackTrace: "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// The display() function also shows the DataFrame, but in a prettier HTML format (this only works in Databricks notebooks)\n",
    "\n",
    "display(pageviewsDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " `printSchema()` prints out the schema, the data types and whether a column can be null:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Compile Error\n",
       "Message: <console>:17: error: not found: value pageviewsDF\n",
       "              pageviewsDF.printSchema()\n",
       "              ^\n",
       "StackTrace: "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pageviewsDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Notice above that the first 2 columns are typed as `Strings`, while the requests column holds `Integers`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Also notice, in a few cells above when we displayed the table, that the rows seem to be missing chunks of time.\n",
    "\n",
    "The first row shows data from March 16, 2015 at **12:09:55am**, and the second row shows data from the same day at **12:10:39am**. There appears to be missing data between those time intervals because the original data file from Wikimedia contains the data out of order and Spark read it into a DataFrame in the same order as the file.\n",
    "\n",
    "Our data set does actually contain 2 rows for every second (one row for mobile site requests and another for desktop site requests).\n",
    "\n",
    "We can verify this by ordering the table by the timestamp column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// The following orders the rows by first the timestamp (ascending), then the site (descending) and then shows the first 10 rows\n",
    "\n",
    "pageviewsDF.orderBy($\"timestamp\", $\"site\".desc).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Reading from disk vs memory\n",
    "\n",
    "The 255 MB pageviews file is currently on S3, which means each time you scan through it, your Spark cluster has to read the 255 MB of data remotely over the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Count how many total records (rows) there are\n",
    "pageviewsDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Hmm, that took about 10 - 20 seconds. Let's cache the DataFrame into memory to speed it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlContext.cacheTable(\"pageviews_by_second\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Caching is a lazy operation (meaning it doesn't take effect until you call an action that needs to read all of the data). So let's call the `count()` action again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// During this count() action, the data is not only read from S3 and counted, but also cached\n",
    "pageviewsDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The DataFrame should now be cached, let's run another `count()` to see the speed increase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pageviewsDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Notice that operating on the DataFrame now takes less than 1 second!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Exploring pageviews\n",
    "\n",
    "Time to do some data analysis!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " \n",
    "### Question #1:\n",
    "**How many rows in the table refer to mobile vs desktop?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pageviewsDF.filter($\"site\" === \"mobile\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pageviewsDF.filter($\"site\" === \"desktop\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " We can also group the data by the `site` column and then call count:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pageviewsDF.groupBy($\"site\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " So, 3.6 million rows refer to the mobile page views and 3.6 million rows refer to desktop page views."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " \n",
    "### Question #2:\n",
    "** How many total incoming requests were to the mobile site vs the desktop site?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " First, let's sum up the `requests` column to see how many total requests are in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Import the sql functions package, which includes statistical functions like sum, max, min, avg, etc.\n",
    "import org.apache.spark.sql.functions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pageviewsDF.select(sum($\"requests\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " So, there are about 13.3 billion requests total."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " But how many of the requests were for the mobile site?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " ** Challenge 1:** Using just the commands we explored above, can you figure out how to filter the DataFrame for just mobile traffic and then sum the requests column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "//Type in your answer here...\n",
    "pageviewsDF.filter(\"site = 'mobile'\").select(sum($\"requests\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " So, that many requests were for the mobile site (and probably came from mobile phone browsers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " ** Challenge 2:** What about the desktop site? How many requests did it get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "//Type in your answer here...\n",
    "pageviewsDF.filter(\"site = 'desktop'\").select(sum($\"requests\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " So, twice as many were for the desktop site."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " \n",
    "### Question #3:\n",
    "** What is the start and end range of time for the pageviews data? How many days of data do we have?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " To accomplish this, we should first convert the `timestamp` column from a `String` type to a `Timestamp` type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Currently in our DataFrame, `pageviewsDF`, the first column is typed as a string\n",
    "pageviewsDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Create a new DataFrame, `pageviewsDF2`, that changes the timestamp column from a `string` data type to a `timestamp` data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val pageviewsDF2 = pageviewsDF.select($\"timestamp\".cast(\"timestamp\").alias(\"timestamp\"), $\"site\", $\"requests\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pageviewsDF2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "display(pageviewsDF2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " How many different years is our data from?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " For the next command, we'll use `year()`, one of the date time function available in Spark. You can review which functions are available for DataFrames in the [Spark API docs](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pageviewsDF2.select(year($\"timestamp\")).distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " The data only spans 2015. But which months?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " ** Challenge 3:** Can you figure out how to check which months of 2015 our data covers (using the Spark API docs linked to above)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "//Type in your answer here...\n",
    "pageviewsDF2.select(month($\"timestamp\")).distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " The data covers the months you see above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " ** Challenge 4:** How many weeks does our data cover?\n",
    "\n",
    "*Hint, check out the Date time functions available in the  [Spark API docs](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "//Type in your answer below...\n",
    "pageviewsDF2.select(weekofyear($\"timestamp\")).distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " The data set covers the number of weeks you see above. Similarly, we can see how many days of coverage we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pageviewsDF2.select(dayofyear($\"timestamp\")).distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " We have 41 days of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " To understand our data better, let's look at the average, minimum and maximum number of requests received for mobile, then desktop page views over every 1 second interval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Look at mobile statistics\n",
    "pageviewsDF2.filter(\"site = 'mobile'\").select(avg($\"requests\"), min($\"requests\"), max($\"requests\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Look at desktop statistics\n",
    "pageviewsDF2.filter(\"site = 'desktop'\").select(avg($\"requests\"), min($\"requests\"), max($\"requests\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " There certainly appears to be more requests for the desktop site."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " \n",
    "### Question #4:\n",
    "** Which day of the week does Wikipedia get the most traffic?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Think about how we can accomplish this. We need to pull out the day of the week (like Mon, Tues, etc) from each row, and then sum up all of the requests by day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " First, use the `date_format` function to extract out the day of the week from the timestamp and rename the column as \"Day of week\".\n",
    "\n",
    "Then we'll sum up all of the requests for each day and show the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Notice the use of alias() to rename the new column\n",
    "// \"E\" is a pattern in the SimpleDataFormat class in Java that extracts out the \"Day in Week\"\"\n",
    "\n",
    "// Create a new DataFrame named pageviewsByDayOfWeekDF and cache it\n",
    "val pageviewsByDayOfWeekDF = pageviewsDF2.groupBy(date_format(($\"timestamp\"), \"E\").alias(\"Day of week\")).sum().cache()\n",
    "\n",
    "// Show what is in the new DataFrame\n",
    "pageviewsByDayOfWeekDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " You can learn more about patterns, like \"E\", that [Java SimpleDateFormat](https://docs.oracle.com/javase/7/docs/api/java/text/SimpleDateFormat.html) allows in the Java Docs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " It would help to visualize the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// This is the same command as above, except here we're tacking on an orderBy() to sort by day of week\n",
    "display(pageviewsByDayOfWeekDF.orderBy($\"Day of week\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Click on the Bar chart icon above to convert the table into a bar chart:\n",
    "\n",
    "#![Bar Chart](http://i.imgur.com/myqoDNV.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Also, under the Plot Options, you may need to set the Keys as \"Day of week\" and the values as \"sum(requests)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Hmm, the ordering of the days of the week is off, because the `orderBy()` operation is ordering the days of the week alphabetically. Instead of that, let's start with Monday and end with Sunday. To accomplish this, we'll need to write a short User Defined Function (UDF) to prepend each `Day of week` with a number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### User Defined Functions\n",
    "\n",
    "A UDF lets you code your own logic for processing column values during a DataFrame query. \n",
    "\n",
    "First, let's create a Scala match expression for pattern matching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def matchDayOfWeek(day:String): String = {\n",
    "  day match {\n",
    "    case \"Mon\" => \"1-Mon\"\n",
    "    case \"Tue\" => \"2-Tue\"\n",
    "    case \"Wed\" => \"3-Wed\"\n",
    "    case \"Thu\" => \"4-Thu\"\n",
    "    case \"Fri\" => \"5-Fri\"\n",
    "    case \"Sat\" => \"6-Sat\"\n",
    "    case \"Sun\" => \"7-Sun\"\n",
    "    case _ => \"UNKNOWN\"\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Test the match expression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "matchDayOfWeek(\"Tue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Great, it works! Now define a UDF named `prependNumber`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val prependNumberUDF = sqlContext.udf.register(\"prependNumber\", (s: String) => matchDayOfWeek(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Note, here is a more idomatic Scala way of registering the same UDF\n",
    "// val prependNumberUDF = sqlContext.udf.register(\"prependNumber\", matchDayOfWeek _)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Test the UDF to prepend the `Day of Week` column in the DataFrame with a number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pageviewsByDayOfWeekDF.select(prependNumberUDF($\"Day of week\")).show(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Our UDF looks like it's working. Next, let's apply the UDF and also order the x axis from Mon -> Sun:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "display((pageviewsByDayOfWeekDF.withColumnRenamed(\"sum(requests)\", \"total requests\")\n",
    "  .select(prependNumberUDF($\"Day of week\"), $\"total requests\")\n",
    "  .orderBy(\"UDF(Day of week)\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Click on the bar chart icon again to convert the above table into a Bar Chart. Also, under the Plot Options, you may need to set the Keys as \"UDF(Day of week)\" and the values as \"total requests\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Wikipedia seems to get significantly more traffic on Mondays than other days of the week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " \n",
    "### Question #5:\n",
    "** Can you visualize both the mobile and desktop site requests in a line chart to compare traffic between both sites by day of the week?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " First, graph the mobile site requests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val mobileViewsByDayOfWeekDF = pageviewsDF2.filter(\"site = 'mobile'\").groupBy(date_format(($\"timestamp\"), \"E\").alias(\"Day of week\")).sum().withColumnRenamed(\"sum(requests)\", \"total requests\").select(prependNumberUDF($\"Day of week\"), $\"total requests\").orderBy(\"UDF(Day of week)\").toDF(\"DOW\", \"mobile_requests\")\n",
    "\n",
    "// Cache this DataFrame\n",
    "mobileViewsByDayOfWeekDF.cache()\n",
    "\n",
    "display(mobileViewsByDayOfWeekDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Click on the bar chart icon again to convert the above table into a Bar Chart. Also, under the Plot Options, you may need to set the Keys as \"DOW\" and the values as \"mobile requests\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Next, graph the desktop site requests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val desktopViewsByDayOfWeekDF = pageviewsDF2.filter(\"site = 'desktop'\").groupBy(date_format(($\"timestamp\"), \"E\").alias(\"Day of week\")).sum().withColumnRenamed(\"sum(requests)\", \"total requests\").select(prependNumberUDF($\"Day of week\"), $\"total requests\").orderBy(\"UDF(Day of week)\").toDF(\"DOW\", \"desktop_requests\")\n",
    "\n",
    "// Cache this DataFrame\n",
    "desktopViewsByDayOfWeekDF.cache()\n",
    "\n",
    "display(desktopViewsByDayOfWeekDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Click on the bar chart icon to convert the above table into a Bar Chart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Now that we have two DataFrames (one for mobile views by day of week and another for desktop views), let's join both of them to create one line chart to visualize mobile vs. desktop page views:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "display(mobileViewsByDayOfWeekDF.join(desktopViewsByDayOfWeekDF, mobileViewsByDayOfWeekDF(\"DOW\") === desktopViewsByDayOfWeekDF(\"DOW\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Click on the line chart icon above to convert the table into a line chart:\n",
    "\n",
    "#![Line Chart](http://i.imgur.com/eXjxL5x.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Then click on Plot Options:\n",
    "\n",
    "#![Plot Options](http://i.imgur.com/sASSo9f.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Finally customize the plot as seen below and click Apply:\n",
    "\n",
    "*(You will have to drag and drop fields from the left pane into either Keys or Values)*\n",
    "\n",
    "#![Customize Plot](http://i.imgur.com/VIyNNoA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Hmm, did you notice that the line chart is a bit deceptive? Beware that it looks like there were almost zero mobile site requests because the y-axis of the line graph starts from 600,000,000 instead of 0.\n",
    "\n",
    "#![Customize Plot](http://i.imgur.com/YiThldl.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Since the y-axis is off, it may appear as if there were almost zero mobile site requests. We can restore a zero baseline by using Matplotlib."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " \n",
    "### Bonus:\n",
    "** Use Matplotlib to fix the line chart visualization above so that the y-axis starts with 0 **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Use the Python notebook for this purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark-DSE Cluster",
   "language": "scala",
   "name": "spark-dse-cluster"
  },
  "language_info": {
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
