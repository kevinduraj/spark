{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add spark streaming kafka to Jupyter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import kafka.serializer.StringDecoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This will take a while.  To add kafka (and S3) to this application, under the covers we ran\n",
    "\n",
    "```\n",
    "dse spark-submit --packages org.apache.spark:spark-streaming-kafka_2.10:1.6.0,org.apache.hadoop:hadoop-aws:2.7.2 ...\n",
    "```\n",
    "\n",
    "The jupyter kernel definition file is here:  See it for yourself. One can always add custom settings to it:\n",
    "\n",
    "`~/.local/share/jupyter/kernels/spark-dse-cluster/kernel.json`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import org.apache.spark._\n",
    "import org.apache.spark.storage._\n",
    "import org.apache.spark.streaming._\n",
    "import org.apache.spark.streaming.kafka._\n",
    "import com.datastax.spark.connector.streaming._\n",
    "import Ordering.StringOrdering\n",
    "import scala.util.Try"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple case class for the stock trades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "case class Trade (\n",
    "stock_symbol:String,\n",
    "exchange:String,\n",
    "trade_timestamp: String,\n",
    "price: Float,\n",
    "quantity: Int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A case class for stats\n",
    "  There is a non-default constructor to make a stats block out of trade.\n",
    "  \n",
    "  The munge function is a reduce that takes 2 stats blocks and gives you the total volume, total price, highest, lowest, first, last, delta over the period, and a list of the trades over the period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "case class Stats (stock_symbol: String,\n",
    "      volume: Int,\n",
    "      total_price: Float,\n",
    "      high: Float,\n",
    "      low: Float,\n",
    "      average: Float,\n",
    "      oldest: Float,\n",
    "      oldest_timestamp: String,\n",
    "      newest: Float,\n",
    "      newest_timestamp: String,\n",
    "      delta: Float,\n",
    "      trades: Seq[String]) {\n",
    "\n",
    "  def this(t:Trade) =\n",
    "    this(stock_symbol = t.stock_symbol,\n",
    "      volume       = t.quantity,\n",
    "      total_price  = t.price,\n",
    "      high         = t.price,\n",
    "      low          = t.price,\n",
    "      oldest       = t.price,\n",
    "      oldest_timestamp = t.trade_timestamp,\n",
    "      newest       = t.price,\n",
    "      newest_timestamp = t.trade_timestamp,\n",
    "      delta        = 0F,\n",
    "      average      = t.price,\n",
    "      trades = Seq(f\"${t.quantity}%d@${t.price}%1.2f\")\n",
    "    )\n",
    "\n",
    "  def munge(r:Stats) = Stats(\n",
    "    stock_symbol  = stock_symbol,\n",
    "    volume        = volume + r.volume,\n",
    "    total_price   = total_price + r.total_price,\n",
    "    high          = high max r.high,\n",
    "    low           = low min r.low,\n",
    "    oldest        = if (oldest_timestamp < r.oldest_timestamp) oldest else r.oldest,\n",
    "    oldest_timestamp = if (oldest_timestamp < r.oldest_timestamp) oldest_timestamp else r.oldest_timestamp,\n",
    "    newest        = if (newest_timestamp > r.newest_timestamp) newest else r.newest,\n",
    "    newest_timestamp = if (newest_timestamp > r.newest_timestamp) newest_timestamp else r.newest_timestamp,\n",
    "    delta         = newest - oldest,\n",
    "    average       = (total_price + r.total_price) / (volume + r.volume),\n",
    "    trades        = trades ++ r.trades)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%cql create keyspace if not exists stock with replication = {'class':'SimpleStrategy','replication_factor':1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr></tr></table>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%cql create table if not exists stock.last_10_seconds( stock_symbol text,\n",
    "volume int,\n",
    "high float,\n",
    "low float,\n",
    "average float,\n",
    "delta float,\n",
    "trades list<text>,\n",
    "primary key (stock_symbol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// The batch interval sets how we collect data for, before analyzing it in a batch\n",
    "val batchInterval = Seconds(5)\n",
    "val windowInterval = Seconds(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Create a new `StreamingContext`, using the SparkContext and batch interval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val ssc = new StreamingContext(sc, batchInterval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Create a Kafka stream\n",
    " \n",
    " ### Note: Get the ip address for the host called kafka, and add it to `/etc/hosts` on all 3 nodes.\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " val directKafkaStream = KafkaUtils.createDirectStream[\n",
    "     String, String, StringDecoder, StringDecoder ](\n",
    "     ssc, Map(\"metadata.broker.list\" ->\"kafka:9092\"), Set(\"Trades\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### map it to a DStrem of Trades\n",
    "\n",
    "Use a simple split to turn a string like this 'foo|bar|baz' into an array.  Use some handy pattern matching to pull the 5 fields out of the array and create an instance of Trade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val trades = directKafkaStream\n",
    "  .map{ case (tid, data) \n",
    "                => data.split('|') match { case Array(ss,ex,dt,p,q)\n",
    "                            => Trade(ss,ex,dt,Try(p.toFloat).getOrElse(0F),Try(q.toInt).getOrElse(0))}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For each DStream reduce it by key and a sliding window.\n",
    "\n",
    "Write that to Cassandra. Use the spark UI (You'll\n",
    "find one at the bottom of the notebook, or use Safari) to look at the DAG. \n",
    "\n",
    "1. How many RDDs do you get in each DStream?\n",
    "2. Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trades\n",
    "      .map(t => (t.stock_symbol, new Stats(t)))\n",
    "      .reduceByKeyAndWindow( _.munge(_) , windowInterval)\n",
    "      .map(_._2)\n",
    "      .saveToCassandra(\"stock\", \"last_10_seconds\",\n",
    "                       SomeColumns(\"stock_symbol\", \"high\", \"low\", \"average\", \"volume\", \"delta\",\"trades\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the Stream.  This has the streaming job run in a background thread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN  2016-02-25 07:14:44,667 org.apache.spark.streaming.StreamingContext: StreamingContext has already been started\n"
     ]
    }
   ],
   "source": [
    "ssc.start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>stock_symbol</th><th>average</th><th>delta</th><th>high</th><th>low</th><th>trades</th><th>volume</th></tr></table>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%cql select * from stock.last_10_seconds where stock_symbol in ('IBM','MSFT','AAPL','GM','F','MMM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Optional just to stop\n",
    "ssc.getActive.foreach { _.stop(stopSparkContext = false) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%cql select * from stock.last_10_seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try this in a terminal:\n",
    "\n",
    "```\n",
    "watch -n 5 \"echo \\\"select stock_symbol, volume, average, low, high, delta from stock.last_10_seconds where stock_symbol in ('IBM','MSFT','AAPL','GM','F','MMM');\\\" | cqlsh node0\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"/terminals/1\" width=1000 height=400/>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%html <iframe src=\"/terminals/1\" width=1000 height=400/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"http://52.36.249.27:4040/stages\" width=1000 height=500/>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val addr = java.net.InetAddress.getByName(\"node0_ext\").getHostAddress\n",
    "kernel.magics.html(s\"\"\"<iframe src=\"http://$addr:4040/stages\" width=1000 height=500/>\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop the streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ssc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark-DSE Cluster",
   "language": "scala",
   "name": "spark-dse-cluster"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
