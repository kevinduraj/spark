{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "// Databricks notebook source exported at Wed, 10 Feb 2016 20:42:12 UTC\n",
    "\n",
    "\n",
    "#![Wikipedia Logo](http://sameerf-dbc-labs.s3-website-us-west-2.amazonaws.com/data/wikipedia/images/w_logo_for_labs.png)\n",
    "\n",
    "# Explore English Wikipedia pageviews by second\n",
    "### Time to complete: 15 minutes\n",
    "\n",
    "#### Business questions:\n",
    "\n",
    "* Question # 1) How many rows in the table refer to *mobile* vs *desktop* site requests?\n",
    "* Question # 2) How many total incoming requests were to the *mobile* site vs the *desktop* site?\n",
    "* Question # 3) What is the start and end range of time for the pageviews data? How many days total of data do we have?\n",
    "* Question # 4) Which day of the week does Wikipedia get the most traffic?\n",
    "* Question #  5) Can you visualize both the mobile and desktop site requests together in a line chart to compare traffic between both sites by day of the week?\n",
    "\n",
    "#### Technical Accomplishments:\n",
    "\n",
    "* Use Spark's Scala and Python APIs\n",
    "* Learn what a `sqlContext` is and how to use it\n",
    "* Load a 255 MB tab separated file into a DataFrame\n",
    "* Cache a DataFrame into memory\n",
    "* Run some DataFrame transformations and actions to create visualizations\n",
    "* Learn the following DataFrame operations: `show()`, `printSchema()`, `orderBy()`, `filter()`, `groupBy()`, `cast()`, `alias()`, `distinct()`, `count()`, `sum()`, `avg()`, `min()`, `max()`\n",
    "* Write a User Defined Function (UDF)\n",
    "* Join two DataFrames\n",
    "* Bonus: Use Matplotlib and Python code within a Scala notebook to create a line chart\n",
    "\n",
    "\n",
    "\n",
    "Dataset: http://datahub.io/en/dataset/english-wikipedia-pageviews-by-second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sqlContext.implicits._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Introduction to running Scala in Databricks Notebooks\n",
    "\n",
    "Place your cursor inside the cells below, one at a time, and hit \"Shift\" + \"Enter\" to execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// This is a Scala cell. You can run normal Scala code here...\n",
    "val x = 1 + 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Here is another Scala cell, that adds 2 to x\n",
    "val y = 2 + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// This line uses string interpolation to prints what y is equal to...\n",
    "println(s\"y is equal to ${y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// You can import additional modules and use them\n",
    "import java.util.Date\n",
    "println(s\"This was last run on: ${new Date}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### DataFrames\n",
    "A `sqlContext` object is your entry point for working with structured data (rows and columns) in Spark.\n",
    "\n",
    "Let's use the `sqlContext` to read a table of the English Wikipedia pageviews per second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Notice that the sqlContext is is actually a HiveContext\n",
    "sqlContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " A `HiveContext` includes additional features like the ability to write queries using the more complete HiveQL parser, access to Hive UDFs, and the ability to read data from Hive tables. In general, you should always aim to use the `HiveContext` over the more limited `sqlContext`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Create a DataFrame named `pageviewsDF` and understand its schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// You need to preload this data into DSE by running a command from a previous notebook.\n",
    "\n",
    "val pageviewsDF = sqlContext\n",
    ".read\n",
    ".format(\"org.apache.spark.sql.cassandra\")\n",
    ".options(Map( \"table\" -> \"pageviews\", \"keyspace\" -> \"pageviews_ks\" ))\n",
    ".load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Shows the first 20 records in ASCII print\n",
    "pageviewsDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " `printSchema()` prints out the schema, the data types and whether a column can be null:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pageviewsDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Notice above that the first 2 columns are typed as `Strings`, while the requests column holds `Integers`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Also notice, in a few cells above when we displayed the table, that the rows seem to be missing chunks of time.\n",
    "\n",
    "The first row shows data from March 16, 2015 at **12:09:55am**, and the second row shows data from the same day at **12:10:39am**. There appears to be missing data between those time intervals because the original data file from Wikimedia contains the data out of order and Spark read it into a DataFrame in the same order as the file.\n",
    "\n",
    "Our data set does actually contain 2 rows for every second (one row for mobile site requests and another for desktop site requests).\n",
    "\n",
    "We can verify this by ordering the table by the timestamp column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// The following orders the rows by first the timestamp (ascending), then the site (descending) and then shows the first 10 rows\n",
    "\n",
    "pageviewsDF.orderBy($\"ts\", $\"site\".desc).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Reading from disk vs memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Count how many total records (rows) there are\n",
    "pageviewsDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Hmm, that took about 10 - 20 seconds. Let's cache the DataFrame into memory to speed it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pageviewsDF.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Caching is a lazy operation (meaning it doesn't take effect until you call an action that needs to read all of the data). So let's call the `count()` action again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// During this count() action, the data is not only read from (*** FIXME ***) S3 and counted, but also cached\n",
    "pageviewsDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The DataFrame should now be cached, let's run another `count()` to see the speed increase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pageviewsDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Notice that operating on the DataFrame now takes less than 1 second!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Exploring pageviews\n",
    "\n",
    "Time to do some data analysis!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " \n",
    "### Question #1:\n",
    "**How many rows in the table refer to mobile vs desktop?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pageviewsDF.filter($\"site\" === \"mobile\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pageviewsDF.filter($\"site\" === \"desktop\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " We can also group the data by the `site` column and then call count:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pageviewsDF.groupBy($\"site\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " So, 3.6 million rows refer to the mobile page views and 3.6 million rows refer to desktop page views."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " \n",
    "### Question #2:\n",
    "** How many total incoming requests were to the mobile site vs the desktop site?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " First, let's sum up the `requests` column to see how many total requests are in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Import the sql functions package, which includes statistical functions like sum, max, min, avg, etc.\n",
    "import org.apache.spark.sql.functions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pageviewsDF.select(sum($\"requests\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " So, there are about 13.3 billion requests total."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " But how many of the requests were for the mobile site?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " ** Challenge 1:** Using just the commands we explored above, can you figure out how to filter the DataFrame for just mobile traffic and then sum the requests column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "//Type in your answer here...\n",
    "pageviewsDF.filter(\"site = 'mobile'\").select(sum($\"requests\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " So, that many requests were for the mobile site (and probably came from mobile phone browsers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " ** Challenge 2:** What about the desktop site? How many requests did it get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "//Type in your answer here...\n",
    "pageviewsDF.filter(\"site = 'desktop'\").select(sum($\"requests\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " So, twice as many were for the desktop site."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " \n",
    "### Question #3:\n",
    "** What is the start and end range of time for the pageviews data? How many days of data do we have?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " To accomplish this, we should first convert the `timestamp` column from a `String` type to a `Timestamp` type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Currently in our DataFrame, `pageviewsDF`, the first column is typed as a string\n",
    "pageviewsDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Create a new DataFrame, `pageviewsDF2`, that changes the timestamp column from a `string` data type to a `timestamp` data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// This probably needs to be \"ts\"\n",
    "\n",
    "val pageviewsDF2 = pageviewsDF.select($\"ts\".cast(\"timestamp\").alias(\"timestamp\"), $\"site\", $\"requests\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pageviewsDF2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pageviewsDF2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " How many different years is our data from?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " For the next command, we'll use `year()`, one of the date time function available in Spark. You can review which functions are available for DataFrames in the [Spark API docs](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pageviewsDF2.select(year($\"timestamp\")).distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " The data only spans 2015. But which months?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " ** Challenge 3:** Can you figure out how to check which months of 2015 our data covers (using the Spark API docs linked to above)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "//Type in your answer here...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " The data covers the months you see above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " ** Challenge 4:** How many weeks does our data cover?\n",
    "\n",
    "*Hint, check out the Date time functions available in the  [Spark API docs](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "//Type in your answer below...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " The data set covers the number of weeks you see above. Similarly, we can see how many days of coverage we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pageviewsDF2.select(dayofyear($\"timestamp\")).distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " We have 41 days of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " To understand our data better, let's look at the average, minimum and maximum number of requests received for mobile, then desktop page views over every 1 second interval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Look at mobile statistics\n",
    "pageviewsDF2.filter(\"site = 'mobile'\").select(avg($\"requests\"), min($\"requests\"), max($\"requests\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Look at desktop statistics\n",
    "pageviewsDF2.filter(\"site = 'desktop'\").select(avg($\"requests\"), min($\"requests\"), max($\"requests\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " There certainly appears to be more requests for the desktop site."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " \n",
    "### Question #4:\n",
    "** Which day of the week does Wikipedia get the most traffic?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Think about how we can accomplish this. We need to pull out the day of the week (like Mon, Tues, etc) from each row, and then sum up all of the requests by day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " First, use the `date_format` function to extract out the day of the week from the timestamp and rename the column as \"Day of week\".\n",
    "\n",
    "Then we'll sum up all of the requests for each day and show the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Notice the use of alias() to rename the new column\n",
    "// \"E\" is a pattern in the SimpleDataFormat class in Java that extracts out the \"Day in Week\"\"\n",
    "\n",
    "// Create a new DataFrame named pageviewsByDayOfWeekDF and cache it\n",
    "val pageviewsByDayOfWeekDF = pageviewsDF2.groupBy(date_format(($\"timestamp\"), \"E\").alias(\"Day of week\")).sum().cache()\n",
    "\n",
    "// Show what is in the new DataFrame\n",
    "pageviewsByDayOfWeekDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " You can learn more about patterns, like \"E\", that [Java SimpleDateFormat](https://docs.oracle.com/javase/7/docs/api/java/text/SimpleDateFormat.html) allows in the Java Docs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " It would help to visualize the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// This is the same command as above, except here we're tacking on an orderBy() to sort by day of week\n",
    "pageviewsByDayOfWeekDF.orderBy($\"Day of week\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Hmm, the ordering of the days of the week is off, because the `orderBy()` operation is ordering the days of the week alphabetically. Instead of that, let's start with Monday and end with Sunday. To accomplish this, we'll need to write a short User Defined Function (UDF) to prepend each `Day of week` with a number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### User Defined Functions\n",
    "\n",
    "A UDF lets you code your own logic for processing column values during a DataFrame query. \n",
    "\n",
    "First, let's create a Scala match expression for pattern matching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def matchDayOfWeek(day:String): String = {\n",
    "  day match {\n",
    "    case \"Mon\" => \"1-Mon\"\n",
    "    case \"Tue\" => \"2-Tue\"\n",
    "    case \"Wed\" => \"3-Wed\"\n",
    "    case \"Thu\" => \"4-Thu\"\n",
    "    case \"Fri\" => \"5-Fri\"\n",
    "    case \"Sat\" => \"6-Sat\"\n",
    "    case \"Sun\" => \"7-Sun\"\n",
    "    case _ => \"UNKNOWN\"\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Test the match expression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matchDayOfWeek(\"Tue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Great, it works! Now define a UDF named `prependNumber`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val prependNumberUDF = sqlContext.udf.register(\"prependNumber\", (s: String) => matchDayOfWeek(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Note, here is a more idomatic Scala way of registering the same UDF\n",
    "// val prependNumberUDF = sqlContext.udf.register(\"prependNumber\", matchDayOfWeek _)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Test the UDF to prepend the `Day of Week` column in the DataFrame with a number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pageviewsByDayOfWeekDF.select(prependNumberUDF($\"Day of week\")).show(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Our UDF looks like it's working. Next, let's apply the UDF and also order the x axis from Mon -> Sun:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// FIXME -- looks like a \n",
    "\n",
    "pageviewsByDayOfWeekDF.withColumnRenamed(\"sum(requests)\", \"total requests\")\n",
    "  .select(prependNumberUDF($\"Day of week\"), $\"total requests\")\n",
    "  .orderBy(\"UDF(Day of week)\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Click on the bar chart icon again to convert the above table into a Bar Chart. Also, under the Plot Options, you may need to set the Keys as \"UDF(Day of week)\" and the values as \"total requests\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Wikipedia seems to get significantly more traffic on Mondays than other days of the week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " \n",
    "### Question #5:\n",
    "** Can you visualize both the mobile and desktop site requests in a line chart to compare traffic between both sites by day of the week?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " First, graph the mobile site requests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val mobileViewsByDayOfWeekDF = pageviewsDF2.filter(\"site = 'mobile'\").groupBy(date_format(($\"timestamp\"), \"E\").alias(\"Day of week\")).sum().withColumnRenamed(\"sum(requests)\", \"total requests\").select(prependNumberUDF($\"Day of week\"), $\"total requests\").orderBy(\"UDF(Day of week)\").toDF(\"DOW\", \"mobile_requests\")\n",
    "\n",
    "// Cache this DataFrame\n",
    "mobileViewsByDayOfWeekDF.cache()\n",
    "\n",
    "mobileViewsByDayOfWeekDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Click on the bar chart icon again to convert the above table into a Bar Chart. Also, under the Plot Options, you may need to set the Keys as \"DOW\" and the values as \"mobile requests\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Next, graph the desktop site requests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val desktopViewsByDayOfWeekDF = pageviewsDF2.filter(\"site = 'desktop'\").groupBy(date_format(($\"timestamp\"), \"E\").alias(\"Day of week\")).sum().withColumnRenamed(\"sum(requests)\", \"total requests\").select(prependNumberUDF($\"Day of week\"), $\"total requests\").orderBy(\"UDF(Day of week)\").toDF(\"DOW\", \"desktop_requests\")\n",
    "\n",
    "// Cache this DataFrame\n",
    "desktopViewsByDayOfWeekDF.cache()\n",
    "\n",
    "desktopViewsByDayOfWeekDF.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Now that we have two DataFrames (one for mobile views by day of week and another for desktop views), let's join both of them to compare mobile vs. desktop page views:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mobileViewsByDayOfWeekDF.join(desktopViewsByDayOfWeekDF, mobileViewsByDayOfWeekDF(\"DOW\") === desktopViewsByDayOfWeekDF(\"DOW\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark-DSE Cluster",
   "language": "scala",
   "name": "spark-dse-cluster"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
