{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add spark streaming kafka to Jupyter\n",
    "\n",
    "We have a set of stock portfolios\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import kafka.serializer.StringDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import org.apache.spark._\n",
    "import org.apache.spark.storage._\n",
    "import org.apache.spark.streaming._\n",
    "import org.apache.spark.streaming.kafka._\n",
    "import com.datastax.spark.connector.streaming._\n",
    "import com.datastax.spark.connector.writer.WriteConf\n",
    "import scala.util.Try"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Case Classes for the trades and portfolios\n",
    "- To make life easy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "case class Trade (\n",
    "stock_symbol:String,\n",
    "exchange:String,\n",
    "trade_timestamp: String,\n",
    "price: Float,\n",
    "quantity: Int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "case class Portfolio (\n",
    "    name: String,\n",
    "    stock_symbol: String,\n",
    "    quantity: Int,\n",
    "    price: Option[Float],\n",
    "    value: Option[Float]\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Roll up the portfolios every 5 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// The batch interval sets how we collect data for, before analyzing it in a batch\n",
    "val BatchInterval = Seconds(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Create a new `StreamingContext`, using the SparkContext and batch interval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val ssc = new StreamingContext(sc, BatchInterval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Create a Kafka stream \n",
    " \n",
    " ### Ensure that each node defines the kafka host on every cluster node.  Get the ip address from someone knowlegeable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " val directKafkaStream = KafkaUtils.createDirectStream[\n",
    "     String, String, StringDecoder, StringDecoder ](\n",
    "     ssc, Map(\"metadata.broker.list\" ->\"kafka:9092\"), Set(\"Trades\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split the strings coming in and turn it into an instance of  Trade "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val trades = directKafkaStream\n",
    "  .map{ case (tid, data) \n",
    "                => data.split('|') match { case Array(ss,ex,dt,p,q)\n",
    "                            => Trade(ss,ex,dt,Try(p.toFloat).getOrElse(0F),Try(q.toInt).getOrElse(0))}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create an RDD for the stock portfolios table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val portfolios = sc.cassandraTable[Portfolio](\"stock\",\"portfolios\").keyBy[String](\"stock_symbol\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For each batch,\n",
    "- You can choose to use either RDDs, SparkSQL, or DataFrames\n",
    "\n",
    "- get the newest trade for each symbol. RDD Hint: Use a reduce for this\n",
    "- join it to portfolios\n",
    "- set the price for the item to the new price\n",
    "- set the value for the item to the new price * quantity\n",
    "- save it to the porfolios table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trades.foreachRDD ( tradesRDD => tradesRDD.map( t => (t.stock_symbol, (t.trade_timestamp,t.price)))\n",
    "                                         .reduceByKey( (l,r) => if (r._1 > l._1) r else l)\n",
    "                                         .join(portfolios)\n",
    "                                         .map{case (stock_symbol,((tt,price), port))\n",
    "                                                    => port.copy(price = Some(price),\n",
    "                                                                 value = Some(port.quantity * price))\n",
    "                                             }\n",
    "                                         .saveToCassandra(\"stock\",\"portfolios\")\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ssc.start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%cql select * from stock.portfolios limit 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put this in the terminal below:\n",
    "\n",
    "```\n",
    " watch \"echo \\\"select * from stock.portfolios where name = 'Ehtel Murakami' ;\\\" | cqlsh node0\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"/terminals/1\" width=1000 height=400/>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%html <iframe src=\"/terminals/1\" width=1000 height=400/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop the stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark-DSE Cluster",
   "language": "scala",
   "name": "spark-dse-cluster"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
