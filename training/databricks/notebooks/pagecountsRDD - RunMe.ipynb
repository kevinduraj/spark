{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "// Databricks notebook source exported at Tue, 9 Feb 2016 20:38:06 UTC\n",
    "\n",
    "\n",
    "#![Wikipedia Logo](http://sameerf-dbc-labs.s3-website-us-west-2.amazonaws.com/data/wikipedia/images/w_logo_for_labs.png)\n",
    "\n",
    "# Analyzing the Wikipedia PageCounts with RDDs\n",
    "### Time to complete: 20 minutes\n",
    "\n",
    "#### Business questions:\n",
    "\n",
    "* Question # 1) How many unique articles in English Wikipedia were requested in this hour?\n",
    "* Question # 2) How many requests total did English Wikipedia get in this hour?\n",
    "* Question # 3) How many requests total did each Wikipedia project get total during this hour?\n",
    "* Question # 4) How many requests did the Apache Spark project recieve during this hour? Which language got the most requests?\n",
    "* Question # 5) How many requests did the English Wiktionary project get during the captured hour?\n",
    "* Question # 6) Which Apache project in English Wikipedia got the most hits during the capture hour?\n",
    "* Question # 7) What were the top 10 pages viewed in English Wikipedia during the capture hour?\n",
    "\n",
    "#### Technical Accomplishments:\n",
    "\n",
    "* Learn how to use the following actions: count, take, takeSample, collect\n",
    "* Learn the following transformations: filter, map, reduceByKey, sortBy\n",
    "* Learn how to cache an RDD and view its number of partitions and total size in memory\n",
    "* Learn how to send a closure function to a map transformation\n",
    "* Learn how to define a case class to organize data in an RDD into objects\n",
    "* Learn how to interpret a DAG visualization and understand the number of stages and tasks\n",
    "* Learn why groupByKey should be avoided\n",
    "\n",
    "\n",
    "\n",
    "Dataset: https://dumps.wikimedia.org/other/pagecounts-raw/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Getting to know the Data\n",
    "How large is the data? Let's use `%fs` to find out. Note: This is not supported on jupyter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "//%fs ls /databricks-datasets/wikipedia-datasets/data-001/pagecounts/sample/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " 589722455  bytes means 589 MB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Note that this file is from Nov 24, 2015 at 17:00 (5pm). It only captures 1 hour of page counts to all of Wikipedia languages and projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### RDDs\n",
    "RDDs can be created by using the Spark Context object's `textFile()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// In Databricks, the SparkContext is already created for you as the variable sc\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val pagecountsRDD = sc.textFile(\"file:///mnt/ephemeral/summitdata/pagecounts-20160210-180000\")\n",
    "pagecountsRDD.first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "If you would like to run it from S3 directly, the credentials are set up.  Replace step 2 with\n",
    "\n",
    "```\n",
    "val pagecountsRDD = sc.textFile(\"s3a://datastaxtraining/summitdata/pagecounts-20160210-180000\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " The `count` action counts how many items (lines) total are in the RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pagecountsRDD.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " So there are about 7.7 million lines. Notice that the `count()` action took 5 - 10 seconds to run b/c it had to scan the entire 589 MB file remotely from S3. This command requires 9 tasks to compute. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " You can use the take action to get the first K records (here K = 10):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pagecountsRDD.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " The take command is much faster because it does not have read the entire file. This command only requires 1 task to compute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Unfortunately this is not very readable because `take()` returns an array and Scala simply prints the array with each element separated by a comma. We can make it prettier by traversing the array to print each record on its own line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pagecountsRDD.take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " In the output above, the first column `aa` is the Wikimedia project name. The following abbreviations are used:\n",
    "```\n",
    "wikibooks: \".b\"\n",
    "wiktionary: \".d\"\n",
    "wikimedia: \".m\"\n",
    "wikipedia mobile: \".mw\"\n",
    "wikinews: \".n\"\n",
    "wikiquote: \".q\"\n",
    "wikisource: \".s\"\n",
    "wikiversity: \".v\"\n",
    "mediawiki: \".w\"\n",
    "```\n",
    "\n",
    "Projects without a period and a following character are Wikipedia projects.\n",
    "\n",
    "The second column is the title of the page retrieved, the third column is the number of requests, and the fourth column is the size of the content returned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Common RDD Transformantions and Actions\n",
    "Next, we'll explore some common transformation and actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " But first, let's cache our base RDD into memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pagecountsRDD.setName(\"pagecountsRDD\").cache.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val addr = java.net.InetAddress.getByName(\"node0_ext\").getHostAddress\n",
    "kernel.magics.html(s\"\"\"<iframe src=\"http://$addr:4040/storage\" width=1000 height=500/>\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "You should now see the RDD in Spark UI's storage tab:\n",
    "#![PagecountsRDD in Storage](http://i.imgur.com/Y3UFJl1.png)\n",
    "\n",
    "Notice that the RDD takes more than 2x the space when cached in memory deserialized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " \n",
    "### Question #1:\n",
    "** How many unique articles in English Wikipedia were requested in this hour?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Let's filter out just the lines referring to English Wikipedia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val enPagecountsRDD = pagecountsRDD.filter { _.startsWith(\"en \") }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Note that the above line is lazy and doesn't actually run the filter. We have to trigger the filter transformation to run by calling an action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "enPagecountsRDD.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " 2.4 million lines refer to the English Wikipedia project. So about half of the 5 million articles in English Wikipedia get requested per hour. Let's take a look at 5 random lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "enPagecountsRDD.takeSample(true, 5).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " \n",
    "### Question #2:\n",
    "** How many requests total did English Wikipedia get in this hour?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Let's define a function, `parse`, to parse out the 4 fields on each line. Then we'll run the parse function on each item in the RDD and create a new RDD named `enPagecountsParsedRDD`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Define a function\n",
    "def parse(line:String) = {\n",
    "  val fields = line.split(' ') //Split the original line with 4 fields according to spaces\n",
    "  (fields(0), fields(1), fields(2).toInt, fields(3).toLong) // return the 4 fields with their correct data types\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " ** Challenge 1:**  Can you use the parse function above in a map closure and assign the results to an RDD named *enPagecountsParsedRDD*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "//Type in your answer here...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "enPagecountsParsedRDD.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Using a combination of `map` and `take`, we can yank out just the requests field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "enPagecountsParsedRDD.map(_._3).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " ** Challenge 2:** Finally, let's sum all of the requests to English Wikipedia during the captured hour:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "//Type in your answer here...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " We can see that there were about 9.6 million requests to English Wikipedia on Nov 24, 2015 from 5pm - 6pm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " \n",
    "### Question #3:\n",
    "** How many requests total did each Wikipedia project get total during this hour?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Recall that our data file contains requests to all of the Wikimedia projects, including Wikibooks, Wiktionary, Wikinews, Wikiquote... and all of the 200+ languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Use the parse function in a map closure\n",
    "val allPagecountsParsedRDD = pagecountsRDD.map(parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allPagecountsParsedRDD.take(5).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Next, we'll create key/value pairs from the project prefix and the number of requests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allPagecountsParsedRDD.map(line => (line._1, line._3)).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Finally, we can use `reduceByKey()` to calculate the final answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val projectcountsRDD = allPagecountsParsedRDD.map(line => (line._1, line._3)).reduceByKey(_ + _)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Sort by the value (number of requests) and pass in false to sort in descending order\n",
    "projectcountsRDD.sortBy(x => x._2, false).take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " We can see that the English Wikipedia Desktop and the English Wikipedia Mobile got the most hits this hour, followed by the Russian and Spanish Wikipedias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " \n",
    "### Question #4:\n",
    "** How many requests did the Apache Spark project recieve during this hour? Which language got the most requests?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " First we define a case class to organize our data in PageCount objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "case class PageCount(val project: String, val title: String, val requests: Long, val size: Long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val pagecountObjectsRDD = pagecountsRDD\n",
    "  .map(_.split(' '))\n",
    "  .filter(_.size == 4)\n",
    "  .map(pc => new PageCount(pc(0), pc(1), pc(2).toLong, pc(3).toLong))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Filter out just the items that mention \"Apache_Spark\" in the title:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pagecountObjectsRDD\n",
    "  .filter(_.title.contains(\"Apache_Spark\"))\n",
    "  .count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " ** Challenge 3:** Can you figure out which language edition of the Apache Spark page got the most hits? \n",
    "\n",
    "Hint: Consider using a .map() after the filter() in the cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "//Type in your answer here...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " It seems like the English version of the Apache Spark page got the most hits by far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " \n",
    "### Question #5:\n",
    "** How many requests did the English Wiktionary project get during the captured hour?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " \n",
    "The [Wiktionary](https://en.wiktionary.org/wiki/Wiktionary:Main_Page) project is a free dictionary with 4 million+ entries from over 1,500 languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " ** Challenge 4:** Can you figure this out? Start by figuring out the correct prefix that identifies the English Wikitionary project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "//Type in your answer here...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " The English Wikionary project got a total of 76,000 requests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " \n",
    "### Question #6:\n",
    "** Which Apache project in English Wikipedia got the most hits during the capture hour?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Here we reuse the PageCount case class we had defined earlier\n",
    "val enPagecountObjectsRDD = enPagecountsRDD\n",
    "  .map(_.split(' '))\n",
    "  .filter(_.size == 4)\n",
    "  .map(pc => new PageCount(pc(0), pc(1), pc(2).toLong, pc(3).toLong))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "enPagecountObjectsRDD\n",
    "  .filter(_.title.contains(\"Apache_\"))\n",
    "  .map(x => (x.title, x.requests))\n",
    "  .collect\n",
    "  .foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "enPagecountObjectsRDD\n",
    "  .filter(_.title.contains(\"Apache_\"))\n",
    "  .map(x => (x.title, x.requests))\n",
    "  .map(item => item.swap) // interchanges position of entries in each tuple\n",
    "  .sortByKey(false, 1) // 1st arg configures ascending sort, 2nd arg configures one task\n",
    "  .map(item => item.swap)\n",
    "  .collect\n",
    "  .foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " We can infer from the above results that Apache's Hadoop and HTTP Server projects are the most popular, followed by Spark and Tomcat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " \n",
    "### Question #7:\n",
    "** What were the top 10 pages viewed in English Wikipedia during the capture hour?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "//Recall that we already have a RDD created that we can use for this analysis\n",
    "enPagecountsParsedRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "enPagecountsParsedRDD\n",
    "  .takeSample(true, 5)\n",
    "  .foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "enPagecountsParsedRDD\n",
    "  .map(line => (line._2, line._3))\n",
    "  .reduceByKey(_ + _)\n",
    "  .sortBy(x => x._2, false)\n",
    "  .take(10)\n",
    "  .foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " The Lucy article sticks out as a unique article that received over 33,000 requests on Nov 24, 2015 between 5pm and 6pm.\n",
    "\n",
    "What could have caused this?\n",
    "\n",
    "On November 24, Google had a special [Google Doodle](https://www.google.com/doodles/41st-anniversary-of-the-discovery-of-lucy) on their main page to celebrate the 41st anniversary of Lucy. There were also a ton of news articles gobally about Lucy on Nov 24."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark-DSE Cluster",
   "language": "scala",
   "name": "spark-dse-cluster"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
